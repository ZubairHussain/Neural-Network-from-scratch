{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "----\n",
    "##Goal is to implement a multi-class Neural network Classifier. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import scipy.stats\n",
    "from collections import defaultdict  # default dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cutomize the Matplotlib for beautiful plots...\n",
    "#comment it if it does not work for you.\n",
    "#import dmStyle\n",
    "#dmStyle.customize_mpl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tools as t # set of tools for plotting, data splitting, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "def GradientDescent(X,Y,cost_function,derivative_cost_function,nparams,maxniter=20000, alpha=0.01, plotcf=True):\n",
    "    '''\n",
    "        Finds the minimum of given cost function using gradient descent.\n",
    "        \n",
    "        Input:\n",
    "        ------\n",
    "            X: can be either a single d X m-dimensional vector \n",
    "                or d X m dimensional matrix of inputs            \n",
    "            \n",
    "            Y: Must be k X m - dimensional label vector\n",
    "            cost_function: a function to be minimized, must return a scalar value\n",
    "            derivative_cost_function: derivative of cost function w.r.t. paramaters, \n",
    "                                       must return partial derivatives w.r.t all d parameters\n",
    "            aparams : number of parameters (length of theta vector to learn...)                                       \n",
    "            maxniter: maximum number of iterations to run\n",
    "            alpha: learning rate\n",
    "            plotcf: If true, plots cost function after every 20 iterations\n",
    "                    \n",
    "        Returns:\n",
    "        ------\n",
    "            thetas: a nclass*d X 1-dimensional vector of cost function parameters \n",
    "                    where minimum point occurs (or location of minimum).\n",
    "    '''\n",
    "    import pdb\n",
    "    # Remember you must plot the cost function after set of iterations to\n",
    "    # check whether your gradient descent code is working fine or not...\n",
    "    eps=0.00001\n",
    "    thetas=rand(nparams,1)\n",
    "    # lets map the thetas to the range [-1,1]\n",
    "    stime=time.time() # Get the Starting time...\n",
    "    \n",
    "    thetas= thetas*2-1  # output = input * range_output  + min_output\n",
    "\n",
    "    theta_old=thetas\n",
    "    theta_new=thetas+1\n",
    "\n",
    "#     pdb.set_trace()\n",
    "    print thetas.shape,theta_old.shape,theta_new.shape\n",
    "    cf=[cost_function(X,Y,theta_old)]\n",
    "#     pai=int(maxniter / 200) # plot in total 200 number of points\n",
    "    #print cf\n",
    "    numiter=0\n",
    "    tnorms=[]\n",
    "    df=ones(thetas.shape)\n",
    "    while numiter < maxniter and np.linalg.norm(df) > eps:\n",
    "        theta_old=theta_new\n",
    "        theta_new=theta_old-alpha*np.array(derivative_cost_function(X,Y,np.array(theta_old)))\n",
    "        numiter+=1\n",
    "        print 'Value of Cost Function at Minimum Points {}, is {}'.format(theta_new,cost_function(X,Y,theta_new))\n",
    "    \n",
    "    # print cf\n",
    "\n",
    "    #print theta_new\n",
    "    print 'Total time taken ={:.4} seconds '.format(time.time()-stime) # difference between current and stored time\n",
    "    return theta_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "def one_of_k(ilabels):\n",
    "    \"\"\"\n",
    "        Computes one of k representation for the given labels\n",
    "        \n",
    "        Input:\n",
    "            ilabels: class labels (m x 1)\n",
    "        \n",
    "        output:\n",
    "             \n",
    "            olabels: one of k representation of \n",
    "    \"\"\"\n",
    "    \n",
    "    classes=np.unique(ilabels)\n",
    "    \n",
    "    nclasses=len(classes)\n",
    "    \n",
    "    olabels=np.zeros((ilabels.shape[0],nclasses))\n",
    "    \n",
    "    for i, l in enumerate(ilabels):        \n",
    "        olabels[i,classes==l]=1\n",
    "        \n",
    "    return olabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A placeholder class \n",
    "# TODO: You have to implement the following class\n",
    "\n",
    "### import pdb\n",
    "## Your code goes here...\n",
    "# You might need to define auxliary classes for composition.. ?\n",
    "\n",
    "class NeuralNetwork:\n",
    "    ''' Implements the multiclass Neural Network For Classification... \n",
    "\n",
    "        It is  a simple implementation of 3 Layer NeuralNetwork, for learning purpose,\n",
    "        you can extend it to multi-layer with some simple modifications....\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self,ifeatdim,nnhlayer,classes,alpha=0.001,maxniter=20000,scalefeatures=False):        \n",
    "        self.thetas=[] # learned set of parameters\n",
    "        self.scalefeatures=scalefeatures\n",
    "        self.classes=np.unique(classes)\n",
    "\n",
    "        self.nclasses=len(self.classes) # dimensions of output\n",
    "        self.ifeatdim=ifeatdim # dimension of input features,\n",
    "        self.nnhlayer=nnhlayer # number of neurons in hidden layer\n",
    "        \n",
    "        self.maxniter=maxniter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        self.sl1theta= (nnhlayer, ifeatdim) # l1theta's shape, remember 1 is already added for bias terms\n",
    "        self.sl2theta= (self.nclasses, nnhlayer+1) # l2thetas's shape, +1 for bias terms\n",
    "\n",
    "        # thetas's for neurons + bias terms..\n",
    "        \n",
    "        self.nthetas= (self.nclasses * nnhlayer+ nnhlayer* self.ifeatdim) + (self.nclasses)\n",
    "        \n",
    "        assert(self.nthetas == np.product(self.sl1theta[:])+np.product(self.sl2theta[:])) # for debugging \n",
    "\n",
    "        pass\n",
    "    \n",
    "    def getnparam(self):\n",
    "        \"\"\"\n",
    "        \n",
    "            returns number of parameters in the network...\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.nthetas\n",
    "    def unroll_thetas(self,thetas):\n",
    "        \"\"\"\n",
    "            unroll (unpacks) a long vector of thetas to respective layer thetas...\n",
    "            we will be using a long vector of thetas during Gradient Descent but\n",
    "            during forward_pass and back_ward pass we will unpack this theta into \n",
    "            thetas of respective layers\n",
    "            \n",
    "            Input:\n",
    "            --------\n",
    "                a long vector of self.nthetas dimensions\n",
    "            Output:\n",
    "                a tuple with first element contains the thetas of first layer\n",
    "                and second element contains the thetas of second layer.\n",
    "        \"\"\"\n",
    "\n",
    "        l1theta= thetas[:np.product(self.sl1theta)].reshape(self.sl1theta)\n",
    "\n",
    "        l2theta= thetas[np.product(self.sl1theta):].reshape(self.sl2theta)\n",
    "\n",
    "        return l1theta, l2theta\n",
    "\n",
    "    def roll_thetas(self,l1theta,l2theta):\n",
    "\n",
    "        \"\"\"\n",
    "            Rolls (packs) the layer-wise thetas to a single long vector of thetas...\n",
    "\n",
    "            Input:\n",
    "            --------\n",
    "                l1theta: thetas of layer-1\n",
    "                l2theta: thetas of layer-2\n",
    "\n",
    "            Output:\n",
    "                a long vector of thetas containing both layers thetas\n",
    "        \"\"\"\n",
    "\n",
    "        return np.concatenate( ( l1theta.ravel(),l2theta.ravel() )  )\n",
    "\n",
    "    def forward_pass(self, X, thetas):\n",
    "        \"\"\"\n",
    "            Compute the forward_pass across the layers...\n",
    "            Input:\n",
    "            X: training matrix (d X m)\n",
    "            thetas: a long vector of thetas\n",
    "        \"\"\"\n",
    "        activations=[]\n",
    "        a2=[]\n",
    "        a3=[]\n",
    "        weightedSums=[]\n",
    "        l1theta,l2theta=self.unroll_thetas(thetas) # unroll the thetas...\n",
    "        # layer 1 computations...\n",
    "        a2=l1theta.dot(X)\n",
    "        # append 1's for the bias terms...\n",
    "        a2=np.vstack((a2,np.ones((1,a2.shape[1]))))\n",
    "        \n",
    "        #layer 2 computations...\n",
    "        a3=l2theta.dot(self.sigmoid(a2))\n",
    "        \n",
    "\n",
    "        return self.sigmoid(a2),self.sigmoid(a3)\n",
    "    def sigmoid(self,z):\n",
    "        \"\"\"\n",
    "            Compute the sigmoid function \n",
    "            Input:\n",
    "                z can be a scalar or a matrix\n",
    "            Returns:\n",
    "                sigmoid of the input variable z\n",
    "        \"\"\"\n",
    "        #clip the z\n",
    "\n",
    "        z = maximum(minimum(16,z),-16) \n",
    "        return 1/(1+np.exp(-z))        \n",
    "    \n",
    "    \n",
    "    def hypothesis(self, X,thetas):\n",
    "        '''\n",
    "            Computes the hypothesis for  given input examples (X) and parameters (thetas).\n",
    "\n",
    "            Input:\n",
    "                X: can be either a single d X n-dimensional vector or d X n dimensional matrix\n",
    "                thetas: Must be a d-dimensional vector\n",
    "            Return:\n",
    "                The computed hypothesis\n",
    "        '''\n",
    "    \n",
    "        return (self.forward_pass(X,thetas))[1]\n",
    "    \n",
    "    def cost_function(self, X,Y, thetas):\n",
    "        '''\n",
    "            Computes the Cost function for given input data (X) and labels (Y).\n",
    "\n",
    "            Input:\n",
    "                X: can be either a single d X n-dimensional vector or d X n dimensional matrix of inputs\n",
    "                theata: must  dk X 1-dimensional vector for representing vectors of k classes\n",
    "                Y: Must be k X n-dimensional label vector\n",
    "                \n",
    "            Return:\n",
    "                Returns the cost of hypothesis with input parameters \n",
    "        '''\n",
    "        m=X.shape[1]\n",
    "        \n",
    "        h= self.hypothesis(X,thetas)\n",
    "        cost=np.sum((-1)*Y*np.log10(h)-(1-Y)*np.log10(1-h))/float(m)\n",
    "        \n",
    "        print \"cost : \",cost\n",
    "        return cost\n",
    "    def derivative_cost_function(self,X,Y,thetas):\n",
    "        '''\n",
    "            Computes the derivates of Cost function w.r.t input parameters (thetas)  \n",
    "            for given input and labels.\n",
    "\n",
    "            Input:\n",
    "            ------\n",
    "                X: can be either a single d X n-dimensional vector or d X n dimensional matrix of inputs\n",
    "                theata: must  dk X 1-dimensional vector for representing vectors of k classes\n",
    "                Y: Must be k X n-dimensional label vector\n",
    "            Returns:\n",
    "            ------\n",
    "                partial_thetas: a dk X 1-dimensional vector of partial derivatives of cost function w.r.t parameters..\n",
    "        '''\n",
    "#       pdb.set_trace()\n",
    "        #forward pass\n",
    "        a2, a3=self.forward_pass(X,thetas)\n",
    "        \n",
    "        #now back-propogate \n",
    "\n",
    "        # unroll thetas\n",
    "        l1theta,l2theta=self.unroll_thetas(thetas) # unroll the thetas...\n",
    "        \n",
    "        nexamples=float(X.shape[1])\n",
    "        \n",
    "        # compute delta3, l2theta\n",
    "        delta3=(a3-Y)\n",
    "        delta3/=nexamples\n",
    "        \n",
    "        derivativeOfDelta3=(delta3.dot(a2.T))\n",
    "        #print derivativeOfDelta3.shape\n",
    "        # compute delta2, l1 theta\n",
    "        delta2=((l2theta.T).dot(delta3))\n",
    "        delta2*=(a2*(1-a2))\n",
    "        delta2=delta2[:-1]\n",
    "        derivativeOfDelta2= delta2.dot(X.T)\n",
    "      \n",
    "        #remember to exclude last element of delta2, representing the deltas of bias terms...\n",
    "        # i.e. delta2=delta2[:-1]\n",
    "        l1theta=derivativeOfDelta2\n",
    "        l2theta=derivativeOfDelta3\n",
    "        \n",
    "        print \"l1theta : \",l1theta.shape\n",
    "        print \"l2theta : \",l2theta.shape\n",
    "\n",
    "        # roll thetas into a big vector\n",
    "        thetas=(self.roll_thetas(l1theta,l2theta)).reshape(thetas.shape) # return the same shape as you received\n",
    "        \n",
    "        return thetas\n",
    "        #return partialderivatives\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        ''' Train LogisticRegression classifier using the given \n",
    "            X [d x m] data matrix and Y labels matrix\n",
    "            \n",
    "            Input:\n",
    "            ------\n",
    "            X: [d x m] a data matrix of m d-dimensional examples.\n",
    "            Y: [k x m] a label vector.\n",
    "            \n",
    "            Returns:\n",
    "            -----------\n",
    "            Nothing\n",
    "            '''\n",
    "        \n",
    "        \n",
    "        nfeatures,nexamples=X.shape\n",
    "        ## now go and train a model for each class...\n",
    "        if self.scalefeatures:\n",
    "            X=self.scale_features(X)\n",
    "        nY=one_of_k(Y.T) # get a n x k\n",
    "        nparams=self.getnparam()\n",
    "        print 'Calling Gradient Descent with following number of parameter={}'.format(nparams)\n",
    "        self.thetas=GradientDescent(X,nY.T,self.cost_function,self.derivative_cost_function,self.getnparam(), self.maxniter,self.alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Test the trained NeuralNetwork classifier result on the given examples X\n",
    "        \n",
    "                   \n",
    "            Input:\n",
    "            ------\n",
    "            X: [d x m] a matrix of m  d-dimensional test examples.\n",
    "           \n",
    "            Returns:\n",
    "            -----------\n",
    "            pclass: the predicted class for the given set of examples, i.e. to which it belongs\n",
    "        \"\"\"\n",
    "        \n",
    "        num_test = X.shape[1]\n",
    "        \n",
    "        if self.sl1theta[1]-X.shape[0] ==1:\n",
    "            # append 1 at the end of each example for the bias term\n",
    "            X=np.vstack((X,np.ones((1,X.shape[1]))))\n",
    "        \n",
    "        if self.scalefeatures:\n",
    "            X=(X-self.xmin)/(self.xmax-self.xmin)\n",
    "        \n",
    "       \n",
    "        Ypred=[]\n",
    "\n",
    "        h=self.hypothesis(X.dot(theta))\n",
    "        pos=np.argmax(h,axis=-1)\n",
    "        Ypred=self.classes[pos]\n",
    "        \n",
    "        return np.array(Ypred).reshape((num_test,1))\n",
    "    \n",
    "    \n",
    "    def scale_features(self,X):\n",
    "        \"\"\"\n",
    "            Normalize each feature to lie in the range [0 ,1]\n",
    "\n",
    "            Input:\n",
    "            ------\n",
    "\n",
    "                X= d x M dimensional data matrix\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "\n",
    "                normalized X\n",
    "        \"\"\"\n",
    "        self.xmin= np.min(X,axis=1)\n",
    "        self.xmax= np.max(X,axis=1)\n",
    "\n",
    "        return (X-self.xmin)/(self.xmax-self.xmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Non-Linear Binary Class Problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some dummy data for testing\n",
    "\n",
    "np.random.seed(seed=99)\n",
    "\n",
    "# make some data up\n",
    "cp=5\n",
    "nclasses=2\n",
    "mean1 = [-cp,-cp]\n",
    "mean2 = [cp,cp]\n",
    "mean3 = [cp,-cp]\n",
    "mean4 = [-cp,cp]\n",
    "cov = [[3.0,0.0],[0.0,3.0]] \n",
    "\n",
    "#create some points\n",
    "nexamples=1000\n",
    "x1 = np.random.multivariate_normal(mean1,cov,nexamples/4)\n",
    "x2 = np.random.multivariate_normal(mean2,cov,nexamples/4)\n",
    "x3 = np.random.multivariate_normal(mean3,cov,nexamples/4)\n",
    "x4 = np.random.multivariate_normal(mean4,cov,nexamples/4)\n",
    "\n",
    "X=np.vstack((x1,x2,x3,x4))\n",
    "Y=np.vstack((1*np.ones((nexamples/4,1)),1*np.ones((nexamples/4,1)),2*np.ones((nexamples/4,1)),2*np.ones((nexamples/4,1))))\n",
    "\n",
    "plt.scatter(x1[:,0],x1[:,1], c='r', s=100)\n",
    "plt.scatter(x2[:,0],x2[:,1], c='r', s=100)            \n",
    "plt.scatter(x3[:,0],x3[:,1], c='b', s=100)\n",
    "plt.scatter(x4[:,0],x4[:,1], c='b', s=100)            \n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"Linear Classification\")\n",
    "plt.xlabel(\"feature $x_1$\")\n",
    "plt.ylabel(\"feature $x_2$\")\n",
    "\n",
    "fig_ml_in_10 = plt.gcf()\n",
    "plt.savefig('multi-linear-class-nn.svg',format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets append a vector of dummy 1's at the end of X to simplify the calculations...\n",
    "X=np.hstack((X,np.ones((X.shape[0],1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ook=one_of_k(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "def test_derivative(X,Y,cost_function,derivative_cost_function,nparams):\n",
    "    \"\"\"\n",
    "        Function test the implementation of the derivative function...\n",
    "        X: Input test examples (m x d)\n",
    "        Y: True labels (m x 1)\n",
    "        cost_function: function used to compute cost function...\n",
    "        derivative_cost_function: function used to compute derivative of the cost function...\n",
    "        nparams: number of parameters in the network...\n",
    "    \"\"\"\n",
    "    thetas=np.random.rand(nparams,1)\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    ad=derivative_cost_function(X,Y,thetas)\n",
    "    print ad\n",
    "    eps=0.0001\n",
    "    cd=[]\n",
    "\n",
    "    for i in range(len(thetas)):\n",
    "        ttpe=thetas.copy()\n",
    "        ttpe[i]=ttpe[i]+eps # add an epsilon for the current theta\n",
    "        ttme=thetas.copy()\n",
    "        ttme[i]=ttme[i]-eps # add an epsilon for the current theta\n",
    "#         print 'diff in direction', ttpe-ttme    \n",
    "        cd.append((cost_function(X,Y,ttpe)-cost_function(X,Y,ttme))/(2*eps))\n",
    "\n",
    "    print 'Computational derivatvie =',cd\n",
    "    print 'Analytical derivative =', ad.shape,ad.T\n",
    "    print 'Their difference=',np.reshape(cd,ad.shape)-ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pdb off\n",
    "nnclass=NeuralNetwork(3,2,[1, 2],alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Shape of Parameters in L1 of ={}\".format(nnclass.sl1theta)\n",
    "print \"Shape of Parameters in L2 of ={}\".format(nnclass.sl2theta)\n",
    "print \"Number of Parameters in Network ={}\".format(nnclass.getnparam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ook.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %reset\n",
    "# we will be sending transpose of the elements to make sure the data is presented in d X m format\n",
    "%pdb off\n",
    "test_derivative(X.T,ook.T,nnclass.cost_function,nnclass.derivative_cost_function,nnclass.getnparam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%pdb\n",
    "#once again sending the transpose of the training and testing matrix\n",
    "nnclass.train(X.T,Y.T) # c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pdb off\n",
    "npts=10000\n",
    "model=nnclass\n",
    "ax=plt.gca()\n",
    "x0spr = max(X[:,0])-min(X[:,0])\n",
    "x1spr = max(X[:,1])-min(X[:,1])\n",
    "\n",
    "tx=np.random.rand(npts,2)\n",
    "tx[:,0] = tx[:,0]*x0spr + min(X[:,0])\n",
    "tx[:,1] = tx[:,1]*x1spr + min(X[:,1])\n",
    "\n",
    "print tx.shape\n",
    "\n",
    "cs= model.predict(tx.T); # send the transpose of X during evaluation as well...\n",
    "print  np.unique(cs)\n",
    "ax.scatter(tx[:,0],tx[:,1],c=cs.ravel(), alpha=.25)\n",
    "\n",
    "ax.hold(True)\n",
    "ax.scatter(X[:,0],X[:,1],\n",
    "              c=list(map(lambda x:'blue' if x==1 else 'lime' if x==2 else 'r',Y)), \n",
    "              linewidth=0,s=25,alpha=1)\n",
    "ax.set_xlim([min(X[:,0]), max(X[:,0])])\n",
    "ax.set_ylim([min(X[:,1]), max(X[:,1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Three class problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some dummy data for training and testing\n",
    "\n",
    "np.random.seed(seed=99)\n",
    "\n",
    "# make some data up using Gaussian distribution...\n",
    "mean1 = [-3,-3]\n",
    "mean2 = [2,2]\n",
    "mean3 = [-4,5]\n",
    "cov = [[1.0,0.0],[0.0,1.0]] \n",
    "\n",
    "#create some points\n",
    "nexamples=600\n",
    "x1 = np.random.multivariate_normal(mean1,cov,nexamples/3)\n",
    "x2 = np.random.multivariate_normal(mean2,cov,nexamples/3)\n",
    "x3 = np.random.multivariate_normal(mean3,cov,nexamples/3)\n",
    "\n",
    "X=np.vstack((x1,x2,x3))\n",
    "Y=np.vstack((1*np.ones((nexamples/3,1)),2*np.ones((nexamples/3,1)),3*np.ones((nexamples/3,1))))\n",
    "\n",
    "plt.scatter(x1[:,0],x1[:,1], c='r', s=100)\n",
    "plt.scatter(x2[:,0],x2[:,1], c='b', s=100)            \n",
    "plt.scatter(x3[:,0],x3[:,1], c='g', s=100)\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"Linear Classification\")\n",
    "plt.xlabel(\"feature $x_1$\")\n",
    "plt.ylabel(\"feature $x_2$\")\n",
    "\n",
    "fig_ml_in_10 = plt.gcf()\n",
    "plt.savefig('multi-linear-class-nn.svg',format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets append a vector of dummy 1's at the end of X to simplify the calculations...\n",
    "X=np.hstack((X,np.ones((X.shape[0],1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ook=one_of_k(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pdb off\n",
    "nnclass=NeuralNetwork(3,2,np.unique(Y),alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %reset\n",
    "%pdb off\n",
    "test_derivative(X.T,ook.T,nnclass.cost_function,nnclass.derivative_cost_function,nnclass.getnparam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnclass.train(X.T,Y.T) # c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let Plots the decision boundaries of the classes, using simple scatter plots... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pdb off\n",
    "npts=10000\n",
    "model=nnclass\n",
    "ax=plt.gca()\n",
    "x0spr = max(X[:,0])-min(X[:,0])\n",
    "x1spr = max(X[:,1])-min(X[:,1])\n",
    "\n",
    "tx=np.random.rand(npts,2)\n",
    "tx[:,0] = tx[:,0]*x0spr + min(X[:,0])\n",
    "tx[:,1] = tx[:,1]*x1spr + min(X[:,1])\n",
    "\n",
    "print tx.shape\n",
    "#pdb.set_trace()\n",
    "cs= model.predict(tx.T);\n",
    "print cs\n",
    "print  np.unique(cs)\n",
    "ax.scatter(tx[:,0],tx[:,1],c=cs.ravel(), alpha=.35)\n",
    "\n",
    "ax.hold(True)\n",
    "ax.scatter(X[:,0],X[:,1],\n",
    "              c=list(map(lambda x:'blue' if x==1 else 'lime' if x==2 else 'r',Y)), \n",
    "              linewidth=0,s=25,alpha=0.75)\n",
    "ax.set_xlim([min(X[:,0]), max(X[:,0])])\n",
    "ax.set_ylim([min(X[:,1]), max(X[:,1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load Iris dataset and test the classifier on them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the data set\n",
    "data=pd.read_csv('./data/iris.data')\n",
    "data.columns=['SepalLength','SepalWidth','PetalLength','PetalWidth','Class']\n",
    "print data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get your data in matrix\n",
    "X=np.asarray(data[['SepalLength','SepalWidth','PetalLength','PetalWidth']].dropna())\n",
    "Y=np.asarray(data['Class'].dropna())\n",
    "print \" Data Set Dimensions=\", X.shape, \" True Class labels dimensions\", Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets append a vector of dummy 1's at the start of X to simplify the calculations...\n",
    "X=np.hstack((X,np.ones((X.shape[0],1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat=[2,3,4]\n",
    "Y=Y.reshape((len(Y),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%pdb\n",
    "nnclass=NeuralNetwork(3,3,np.unique(Y),alpha=0.02)\n",
    "nnclass.train(X[:,feat].T,Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(plabels,tlabels):\n",
    "    \"\"\"\n",
    "        functions print the confusion matrix for the different classes\n",
    "        to find the error...\n",
    "        \n",
    "        Input:\n",
    "        -----------\n",
    "        plabels: predicted labels for the classes...\n",
    "        tlabels: true labels for the classes\n",
    "        \n",
    "        code from: http://stackoverflow.com/questions/2148543/how-to-write-a-confusion-matrix-in-python\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    plabels = pd.Series(plabels)\n",
    "    tlabels = pd.Series(tlabels)\n",
    "    \n",
    "    # draw a cross tabulation...\n",
    "    df_confusion = pd.crosstab(tlabels,plabels, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    #print df_confusion\n",
    "    return df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_confusion_matrix((nnclass.predict(X[:,feat].T)).ravel(),Y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lets plot the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "npts=10000\n",
    "model=nnclass\n",
    "ax=plt.gca()\n",
    "x0spr = max(X[:,feat[0]])-min(X[:,feat[0]])\n",
    "x1spr = max(X[:,feat[1]])-min(X[:,feat[1]])\n",
    "\n",
    "tx=np.random.rand(npts,2)\n",
    "tx[:,0] = tx[:,0]*x0spr + min(X[:,feat[0]])\n",
    "tx[:,1] = tx[:,1]*x1spr + min(X[:,feat[1]])\n",
    "\n",
    "print tx.shape\n",
    "#pdb.set_trace()\n",
    "cs= model.predict(tx.T);\n",
    "# print cs\n",
    "print  np.unique(cs)\n",
    "mc=list(map(lambda x:1 if x=='Iris-setosa' else 2 if x=='Iris-versicolor' else 3,cs))\n",
    "ax.scatter(tx[:,0],tx[:,1],c=mc, alpha=.25)\n",
    "\n",
    "ax.hold(True)\n",
    "ax.scatter(X[:,feat[0]],X[:,feat[1]],\n",
    "               c=list(map(lambda x:'blue' if x=='Iris-setosa' else 'lime' if x=='Iris-versicolor' else 'r',Y)), \n",
    "               linewidth=0,s=25,alpha=0.8)\n",
    "ax.set_xlim([min(X[:,feat[0]]), max(X[:,feat[0]])])\n",
    "ax.set_ylim([min(X[:,feat[1]]), max(X[:,feat[1]])])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
